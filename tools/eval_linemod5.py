from importlib.abc import Loader
from sys import prefix

from yaml import FullLoader

import _init_paths
import argparse
import os
import random
import numpy as np
import yaml
import copy
import torch
import torch.nn as nn
import torch.nn.parallel
import cv2
import torch.backends.cudnn as cudnn
import torch.optim as optim
import torch.utils.data
import torchvision.datasets as dset
import torchvision.transforms as transforms
import torchvision.utils as vutils
from torch.autograd import Variable
from datasets.linemod5_eval.dataset import PoseDataset as PoseDataset_linemod5
from lib.network import PoseNet, PoseRefineNet
from lib.loss import Loss
from lib.loss_refiner import Loss_refine
from lib.transformations import euler_matrix, quaternion_matrix, quaternion_from_matrix
from lib.knn.__init__ import KNearestNeighbor
import matplotlib.pyplot as plt
from utils.visualization import draw_coordinate_axis, draw_3d_bbox, get_3d_obb_corners
import cv2
import time

from datetime import datetime


parser = argparse.ArgumentParser()
parser.add_argument('--dataset_root', type=str, default = '/media/q/SSD2T/1linux/Linemod5_dataset/Linemod_preprocessed', help='dataset root dir')
parser.add_argument('--model', type=str, default = '/media/q/SSD2T/1linux/Linemod5_trained_mod/pose_model_3_0.007043762697527806.pth',  help='resume PoseNet model')
parser.add_argument('--refine_model', type=str, default = '/media/q/SSD2T/1linux/Linemod5_trained_mod/pose_refine_model_5_0.005734924412487696.pth',  help='resume PoseRefineNet model')
opt = parser.parse_args()

cx = 2038.63190
cy = 1408.83467
fx = 5070.07988
fy = 5142.08286

camera_matrix = np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])
    # dist_coeffs = np.array(
    # [camera_data['k1'], camera_data['k2'], camera_data['p1'], camera_data['p2'], camera_data['k3']]) # ç›¸æœºç•¸å˜å‚æ•°
    # æ–°å¢å¯è§†åŒ–å‚æ•°
vis_size = (4000, 2700)  # å¯è§†åŒ–å›¾åƒå°ºå¯¸
axis_length = 0.1  # åæ ‡ç³»è½´é•¿åº¦ï¼ˆç±³ï¼‰
bbox_color = (0, 255, 122)  # åŒ…å›´æ¡†é¢œè‰² (BGR)
bbox_color2 = (255, 0,0)  # åŒ…å›´æ¡†é¢œè‰² (BGR)
axis_colors = [(0, 0, 255), (0, 255, 0), (255, 0, 0)]  # XYZè½´é¢œè‰²

#æ¨¡å‹çš„å‚æ•°
num_objects = 1
objlist = [3]
num_points = 1000
iteration =2
bs = 1

dataset_config_dir = '/media/q/SSD2T/1linux/Linemod5_dataset/dataset_config'
output_result_dir = '/media/q/SSD2T/1linux/Linemod5_eval'
knn = KNearestNeighbor(1)

# ç½‘ç»œå’Œæ¨¡å‹åŠ è½½
estimator = PoseNet(num_points = num_points, num_obj = num_objects)
estimator.cuda()
refiner = PoseRefineNet(num_points = num_points, num_obj = num_objects)
refiner.cuda()

estimator.load_state_dict(torch.load(opt.model), strict=True)
refiner.load_state_dict(torch.load(opt.refine_model), strict=True)

estimator.eval()
refiner.eval()
# Estimator å‚æ•°é‡
estimator_params = sum(p.numel() for p in estimator.parameters() if p.requires_grad)
print("Estimator Params:", estimator_params)

# Refiner å‚æ•°é‡
refiner_params = sum(p.numel() for p in refiner.parameters() if p.requires_grad)
print("Refiner Params:", refiner_params)

# æ•°æ®åŠ è½½å™¨å’Œè¯„ä¼°å™¨
testdataset = PoseDataset_linemod5('eval', num_points, False, opt.dataset_root, 0.0, True)
testdataloader = torch.utils.data.DataLoader(testdataset, batch_size=1, shuffle=False, num_workers=10)

sym_list = testdataset.get_sym_list()
num_points_mesh = testdataset.get_num_points_mesh()
criterion = Loss(num_points_mesh, sym_list)
criterion_refine = Loss_refine(num_points_mesh, sym_list)

diameter = []
meta_file = open('{0}/models_info.yml'.format(dataset_config_dir), 'r')
meta = yaml.load(meta_file, Loader=FullLoader)
for obj in objlist:
    diameter.append(meta[obj]['diameter'] / 1000.0 * 0.01)
print(diameter)

success_count = [0 for i in range(num_objects)]
num_count = [0 for i in range(num_objects)]
fw = open('{0}/eval_result_logs.txt'.format(output_result_dir), 'w')

# åŸºç¡€ç›®å½•
base_dir = "/media/q/SSD2T/1linux/Linemod5_eval/evaluation_result/"

# ç”Ÿæˆæ—¶é—´æˆ³ï¼Œæ¯”å¦‚ï¼š2025-09-14_14-55-32
timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

# åœ¨ base_dir ä¸‹é¢æ–°å»ºä¸€ä¸ªæ—¶é—´æˆ³å­ç›®å½•
save_dir = os.path.join(base_dir, timestamp)
os.makedirs(save_dir, exist_ok=True)

print(f"ç»“æœä¿å­˜ç›®å½•: {save_dir}")
import time

total_frames = len(testdataset)
start_time = time.time()

print('Start inference...')
for i, data in enumerate(testdataloader, 0):
    points, choose, img, target, model_points, idx , ori_img = data
    if len(points.size()) == 2:
        print('No.{0} NOT Pass! Lost detection!'.format(i))
        fw.write('No.{0} NOT Pass! Lost detection!\n'.format(i))
        continue
    points, choose, img, target, model_points, idx ,ori_img = points.cuda(), \
                                                     choose.cuda(), \
                                                     img.cuda(), \
                                                     target.cuda(), \
                                                     model_points.cuda(), \
                                                     idx.cuda(), \
                                                     ori_img.cuda()

    # ğŸ”¹ åæŸ¥åŸå§‹ RGB è·¯å¾„______________________________________________________________________
    rgb_path = testdataset.list_rgb[i]
    frame_id = os.path.splitext(os.path.basename(rgb_path))[0]
    print(f"å½“å‰æ ·æœ¬åŸå§‹å¸§å·: {frame_id}")

    torch.cuda.empty_cache()
    with torch.no_grad():
        # ç¬¬ä¸€æ­¥ï¼šPoseNet æ¨ç†ï¼ˆestimatorï¼‰
        pred_r, pred_t, pred_c, emb = estimator(img, points, choose, idx)
        pred_r = pred_r / torch.norm(pred_r, dim=2).view(1, num_points, 1) # å¯¹ pred_r åœ¨ç¬¬ 2 ç»´ä¸Šè®¡ç®— L2 èŒƒæ•°ï¼Œå¹¶å°†å…¶å½’ä¸€åŒ–ï¼Œç¡®ä¿æ—‹è½¬è¡¨ç¤ºä¿æŒå•ä½é•¿åº¦ï¼Œ
        pred_c = pred_c.view(bs, num_points)  # å°†ç½®ä¿¡åº¦ pred_c é‡å¡‘ä¸ºå½¢çŠ¶ä¸º (bs, num_points) çš„äºŒç»´å¼ é‡
        how_max, which_max = torch.max(pred_c, 1) # æ‰¾å‡ºæ¯ä¸ªæ‰¹æ¬¡ä¸­æ¯ä¸ªæ ·æœ¬çš„æœ€å¤§ç½®ä¿¡åº¦å€¼ï¼ˆhow_maxï¼‰åŠå…¶å¯¹åº”çš„ç´¢å¼•ï¼ˆwhich_maxï¼‰
        pred_t = pred_t.view(bs * num_points, 1, 3) # å°†å¹³ç§»é¢„æµ‹ pred_t é‡å¡‘ä¸ºå½¢çŠ¶ä¸º (bs * num_points, 1, 3) çš„å¼ é‡

        # ç¬¬äºŒæ­¥ï¼šrefiner è¿­ä»£ç»†åŒ–
        my_r = pred_r[0][which_max[0]].view(-1).cpu().data.numpy() # å±•å¹³
        my_t = (points.view(bs * num_points, 1, 3) + pred_t)[which_max[0]].view(-1).cpu().data.numpy() # å±•å¹³
        my_pred = np.append(my_r, my_t) # ç»„åˆ
        for ite in range(0, iteration):
            T = torch.from_numpy(my_t.astype(np.float32)).cuda().view(1, 3).repeat(num_points,1).contiguous().view(1,num_points,3) # å¹³ç§»çŸ©é˜µç§»åŠ¨åˆ°gpu
            my_mat = quaternion_matrix(my_r) # æ—‹è½¬å››å…ƒæ•°è½¬çŸ©é˜µ
            R = torch.from_numpy(my_mat[:3, :3].astype(np.float32)).cuda().view(1, 3, 3) # æ—‹è½¬è½¬gpu
            my_mat[0:3, 3] = my_t # å¹³ç§»åŠ åˆ°çŸ©é˜µ

            new_points = torch.bmm((points - T), R).contiguous() # ç»è¿‡åç§»åçš„æ–°ç‚¹äº‘
            pred_r, pred_t = refiner(new_points, emb, idx) # ç»†åŒ–ç½‘ç»œ
            pred_r = pred_r.view(1, 1, -1) # æ”¹tenserå½¢çŠ¶
            pred_r = pred_r / (torch.norm(pred_r, dim=2).view(1, 1, 1)) #  å½’ä¸€åŒ– å¤„ç†
            my_r_2 = pred_r.view(-1).cpu().data.numpy()
            my_t_2 = pred_t.view(-1).cpu().data.numpy() # è¿™ä¸¤ä¸ªå¼ é‡è½¬æ¢ä¸º NumPy æ•°ç»„
            my_mat_2 = quaternion_matrix(my_r_2) # # æ—‹è½¬å››å…ƒæ•°è½¬çŸ©é˜µ
            my_mat_2[0:3, 3] = my_t_2 # å¹³ç§»åŠ åˆ°çŸ©é˜µ

            my_mat_final = np.dot(my_mat, my_mat_2) # çŸ©é˜µä¹˜ç§¯
            my_r_final = copy.deepcopy(my_mat_final) # æ·±æ‹·è´
            my_r_final[0:3, 3] = 0 # å°† my_r_final çŸ©é˜µçš„å‰ 3 è¡Œï¼ˆç¬¬ 0 è¡Œåˆ°ç¬¬ 2 è¡Œï¼‰ä¸­çš„ç¬¬ 3 åˆ—çš„æ‰€æœ‰å…ƒç´ è®¾ä¸º 0
            my_r_final = quaternion_from_matrix(my_r_final, True) # å°†å˜æ¢çŸ©é˜µ my_r_final ä» 4x4 çŸ©é˜µè½¬æ¢ä¸ºå››å…ƒæ•°è¡¨ç¤º
            my_t_final = np.array([my_mat_final[0][3], my_mat_final[1][3], my_mat_final[2][3]]) # ä» my_mat_final ä¸­æå–å¹³ç§»éƒ¨åˆ†

            my_pred = np.append(my_r_final, my_t_final) # å°†æ—‹è½¬å››å…ƒæ•° my_r_final å’Œä½ç§»å‘é‡ my_t_final åˆå¹¶ä¸ºä¸€ä¸ªæ•°ç»„ my_pred
            my_r = my_r_final
            my_t = my_t_final

     #Here 'my_pred' is the final pose estimation result after refinement ('my_r': quaternion, 'my_t': translation)

        # ç¬¬ä¸‰æ­¥ï¼šåå¤„ç†ï¼ˆåˆšä½“å˜æ¢ã€è®¡ç®— dis ç­‰ï¼‰
        model_points = model_points[0].cpu().detach().numpy() # ä» PyTorch å¼ é‡è½¬æ¢ä¸º NumPy æ•°ç»„
        my_r = quaternion_matrix(my_r)[:3, :3] # å°†å››å…ƒæ•° my_r è½¬æ¢ä¸ºæ—‹è½¬çŸ©é˜µï¼Œå¹¶æå–å‡ºæ—‹è½¬çŸ©é˜µçš„å‰ä¸‰è¡Œå‰ä¸‰åˆ—
        pred = np.dot(model_points, my_r.T) + my_t
        # model_points æ˜¯ç‚¹äº‘æ•°æ®
        # my_r.T æ˜¯æ—‹è½¬çŸ©é˜µ my_r çš„è½¬ç½®
        # my_t æ˜¯å¹³ç§»å‘é‡
        # å¯¹model_points æ‰§è¡Œ åˆšæ€§å˜æ¢
        target = target[0].cpu().detach().numpy() # å°†ä¸€ä¸ªå¼ é‡ target ä» PyTorch ä¸­çš„è®¡ç®—å›¾ä¸­åˆ†ç¦»å‡ºæ¥ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸º NumPy æ•°ç»„

        if idx[0].item() in sym_list:
            pred = torch.from_numpy(pred.astype(np.float32)).cuda().transpose(1, 0).contiguous()
            # å°†ä¸€ä¸ª NumPy æ•°ç»„ pred è½¬æ¢ä¸º PyTorch å¼ é‡ï¼Œå¹¶åšä¸€äº›å¼ é‡æ“ä½œï¼Œæœ€ç»ˆå¾—åˆ°ä¸€ä¸ªé€‚åˆ GPU è®¡ç®—çš„å¼ é‡
            target = torch.from_numpy(target.astype(np.float32)).cuda().transpose(1, 0).contiguous()
            # å°† NumPy æ•°ç»„ target è½¬æ¢ä¸º PyTorch å¼ é‡
            inds = knn.forward(knn.k, target.unsqueeze(0), pred.unsqueeze(0))
            # æ¥æ‰§è¡Œ k æœ€è¿‘é‚»ï¼ˆKNNï¼‰æ“ä½œï¼›kç”¨äºæŒ‡å®šæ¯æ¬¡æŸ¥è¯¢æ—¶è¿”å›å¤šå°‘ä¸ªé‚»å±…
            target = torch.index_select(target, 1, inds.view(-1) - 1)

            dis = torch.mean(torch.norm((pred.transpose(1, 0) - target.transpose(1, 0)), dim=1), dim=0).item()

        else:
            dis = np.mean(np.linalg.norm(pred - target, axis=1))

        if dis < diameter[idx[0].item()]:
            success_count[idx[0].item()] += 1
            print('No.{0} Pass! Distance: {1}'.format(i, dis))
            fw.write('No.{0} Pass! Distance: {1}\n'.format(i, dis))
        else:
            print('No.{0} NOT Pass! Distance: {1}'.format(i, dis))
            fw.write('No.{0} NOT Pass! Distance: {1}\n'.format(i, dis))
        num_count[idx[0].item()] += 1

    ###############################################################################################
    #                                              å¯è§†åŒ–                                      #####
    ###############################################################################################
    pose = np.eye(4)
    pose[:3, :3] = my_r  # æ—‹è½¬çŸ©é˜µ (3x3)
    pose[:3, 3] = my_t.squeeze()  # å¹³ç§»å‘é‡ (3,)


    np_img = ori_img[0].cpu().numpy() # å›¾åƒå‡†å¤‡
    print("Original shape:", np_img.shape)
    # np_img = np.transpose(np_img, (2,0,1))  # å˜æˆ (H, W, 3)
    np_img = 255 - np_img # å›¾åƒåè½¬ï¼ˆæ­£ç‰‡æ¢å¤ï¼‰
    np_img = (np_img * 255).astype(np.uint8) # numpyè½¬ä¸º0-255

    # ------------------------- #
    # å¯è§†åŒ–åæ ‡è½´å’ŒåŒ…å›´æ¡†ç»˜åˆ¶éƒ¨åˆ†
    # ------------------------- #
    img_draw = np_img.copy() # å¤åˆ¶ä¸€ä»½å›¾åƒç”¨äºç”»å›¾ï¼ˆé¿å…åŸå›¾å—ä¿®æ”¹ï¼‰
    # å¯è§†åŒ–1ï¼šç»˜åˆ¶åæ ‡ç³»
    draw_coordinate_axis(img_draw, pose, camera_matrix,axis_length, axis_colors) # ç”»å‡ºåæ ‡ç³»
    # å¯è§†åŒ–2ï¼šç»˜åˆ¶3DåŒ…å›´æ¡†
    bbox_corners = get_3d_obb_corners(model_points) # è·å–gtæ¨¡å‹çš„3D Box
    draw_3d_bbox(img_draw, bbox_corners, pose, camera_matrix,color=bbox_color, thickness=1) # ç”»å‡ºåŒ…å›´ç›’

    bbox_corners2 = get_3d_obb_corners(target) # è·å–gtæ¨¡å‹çš„3D Box
    draw_3d_bbox(img_draw, bbox_corners2, np.eye(4), camera_matrix,color=bbox_color2, thickness=1) # ç”»å‡ºåŒ…å›´ç›’
    print("åŒ…å›´ç›’è§’ç‚¹åæ ‡ (å•ä½: ç±³):\n", bbox_corners)
    print("åŒ…å›´ç›’å°ºå¯¸ (X, Y, Z):",
          np.ptp(bbox_corners[:, 0]),  # Xè½´é•¿åº¦
          np.ptp(bbox_corners[:, 1]),  # Yè½´é•¿åº¦
          np.ptp(bbox_corners[:, 2]))  # Zè½´é•¿åº¦
    # ------------------------- #
    # ç”¨ Matplotlib æ˜¾ç¤ºæœ€ç»ˆå›¾åƒ
    # ------------------------- #
    from datetime import datetime
    plt.figure(figsize=(8, 6))
    save_path = os.path.join(save_dir, f"pose_visualization_{frame_id}.png")

    plt.imsave(save_path, img_draw)
    print(f"å›¾åƒå·²ä¿å­˜è‡³: {save_path}")
    plt.close()
    #########################################################


end_time = time.time()
total_time = end_time - start_time

fps = total_frames / total_time
print(f"ç«¯åˆ°ç«¯ FPS: {fps:.2f}")

for i in range(num_objects):
    print('Object {0} success rate: {1}'.format(objlist[i], float(success_count[i]) / num_count[i]))
    fw.write('Object {0} success rate: {1}\n'.format(objlist[i], float(success_count[i]) / num_count[i]))
print('ALL success rate: {0}'.format(float(sum(success_count)) / sum(num_count)))
fw.write('ALL success rate: {0}\n'.format(float(sum(success_count)) / sum(num_count)))
fw.close()
